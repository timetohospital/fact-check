#!/usr/bin/env python3
"""
AI ê¸°ë°˜ ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„ ë° A/B í…ŒìŠ¤íŠ¸ ìƒì„±

1. ì„±ê³¼ í•˜ìœ„ 20% ê¸€ ì¶”ì¶œ
2. AIë¡œ ì›ì¸ ë¶„ì„ + ê°€ì„¤ ìƒì„±
3. B ë²„ì „ ìë™ ìƒì„±
4. A/B í…ŒìŠ¤íŠ¸ ë“±ë¡

ì‹¤í–‰: python scripts/analyze_and_create_ab.py
ìŠ¤ì¼€ì¤„: 3ì¼ë§ˆë‹¤ (GA ìˆ˜ì§‘ ì§í›„)
"""

import os
import json
import uuid
from datetime import datetime, timedelta
from typing import Optional
import psycopg2
from openai import OpenAI

# ============================================
# ì„¤ì •
# ============================================

DB_CONFIG = {
    "host": "34.64.111.186",
    "port": 5432,
    "user": "admin",
    "password": "galddae-password",
    "database": "factcheck_db"
}

# OpenAI ì„¤ì •
OPENAI_MODEL = "gpt-4o"

# ë¶„ì„ ì„¤ì •
UNDERPERFORMING_PERCENTILE = 20  # í•˜ìœ„ 20%
MAX_AB_TESTS_PER_RUN = 3  # í•œ ë²ˆì— ìµœëŒ€ 3ê°œ í…ŒìŠ¤íŠ¸ ìƒì„±


def get_db_connection():
    """DB ì—°ê²°"""
    return psycopg2.connect(**DB_CONFIG)


def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸"""
    return OpenAI()


def get_underperforming_articles(cursor) -> list:
    """
    ì„±ê³¼ í•˜ìœ„ 20% ê¸€ ì¶”ì¶œ

    ê¸°ì¤€: ìµœê·¼ 7ì¼ í‰ê·  engagement_score
    """
    cursor.execute("""
        WITH article_stats AS (
            SELECT
                article_slug,
                AVG(engagement_score) as avg_engagement,
                AVG(avg_time_on_page) as avg_time,
                AVG(bounce_rate) as avg_bounce,
                SUM(pageviews) as total_views
            FROM article_metrics
            WHERE date >= CURRENT_DATE - INTERVAL '7 days'
            GROUP BY article_slug
            HAVING SUM(pageviews) >= 10  -- ìµœì†Œ 10 PV ì´ìƒ
        ),
        percentile AS (
            SELECT PERCENTILE_CONT(0.2) WITHIN GROUP (ORDER BY avg_engagement) as p20
            FROM article_stats
        )
        SELECT
            s.article_slug,
            s.avg_engagement,
            s.avg_time,
            s.avg_bounce,
            s.total_views,
            a.title,
            a.sections,
            a.category
        FROM article_stats s
        JOIN articles a ON s.article_slug = a.slug AND a.version = 'A'
        CROSS JOIN percentile p
        WHERE s.avg_engagement < p.p20
        ORDER BY s.avg_engagement ASC
        LIMIT %s
    """, (MAX_AB_TESTS_PER_RUN,))

    columns = [desc[0] for desc in cursor.description]
    return [dict(zip(columns, row)) for row in cursor.fetchall()]


def get_top_performing_articles(cursor) -> list:
    """ì„±ê³¼ ìƒìœ„ 20% ê¸€ ì¶”ì¶œ (ë¹„êµ ì°¸ê³ ìš©)"""
    cursor.execute("""
        WITH article_stats AS (
            SELECT
                article_slug,
                AVG(engagement_score) as avg_engagement,
                AVG(avg_time_on_page) as avg_time,
                AVG(bounce_rate) as avg_bounce
            FROM article_metrics
            WHERE date >= CURRENT_DATE - INTERVAL '7 days'
            GROUP BY article_slug
            HAVING SUM(pageviews) >= 10
        ),
        percentile AS (
            SELECT PERCENTILE_CONT(0.8) WITHIN GROUP (ORDER BY avg_engagement) as p80
            FROM article_stats
        )
        SELECT
            s.article_slug,
            s.avg_engagement,
            a.sections
        FROM article_stats s
        JOIN articles a ON s.article_slug = a.slug AND a.version = 'A'
        CROSS JOIN percentile p
        WHERE s.avg_engagement >= p.p80
        ORDER BY s.avg_engagement DESC
        LIMIT 3
    """)

    columns = [desc[0] for desc in cursor.description]
    return [dict(zip(columns, row)) for row in cursor.fetchall()]


def analyze_article(client: OpenAI, article: dict, top_articles: list) -> dict:
    """
    AIë¡œ ê¸€ ë¶„ì„ ë° ê°œì„  ê°€ì„¤ ìƒì„±

    Returns:
        {
            "problems": [...],
            "hypothesis": {...},
            "improved_sections": {...}
        }
    """
    sections = article.get("sections", [])
    if isinstance(sections, str):
        sections = json.loads(sections)

    top_sections_summary = ""
    for top in top_articles[:2]:
        top_sec = top.get("sections", [])
        if isinstance(top_sec, str):
            top_sec = json.loads(top_sec)
        if top_sec and len(top_sec) > 0:
            intro = next((s for s in top_sec if s.get("type") == "intro"), top_sec[0])
            top_sections_summary += f"\n- ì¸ê¸°ê¸€ ({top['article_slug']}): {intro.get('content', '')[:200]}..."

    prompt = f"""ë‹¹ì‹ ì€ ì½˜í…ì¸  ì„±ê³¼ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ë¶„ì„ ëŒ€ìƒ ê¸€
- ì œëª©: {article.get('title', 'N/A')}
- ì¹´í…Œê³ ë¦¬: {article.get('category', 'N/A')}
- í‰ê·  ì²´ë¥˜ì‹œê°„: {article.get('avg_time', 0):.1f}ì´ˆ
- ì´íƒˆë¥ : {article.get('avg_bounce', 0):.1f}%
- ì°¸ì—¬ ì ìˆ˜: {article.get('avg_engagement', 0):.1f}/100 (í•˜ìœ„ 20%)

## í˜„ì¬ ë³¸ë¬¸ êµ¬ì¡°
{json.dumps(sections, ensure_ascii=False, indent=2)}

## ì°¸ê³ : ì„±ê³¼ ì¢‹ì€ ê¸€ì˜ ë„ì…ë¶€
{top_sections_summary}

## ë¶„ì„ ìš”ì²­

1. **ë¬¸ì œì  ì§„ë‹¨**: ì„±ê³¼ê°€ ë‚®ì€ ì´ìœ ë¥¼ 3ê°€ì§€ ì´ë‚´ë¡œ ë¶„ì„í•˜ì„¸ìš”.
   - intro ì„¹ì…˜ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§€ë£¨í•œê°€?
   - í•µì‹¬ ê²°ë¡ ì´ ëŠ¦ê²Œ ë‚˜ì˜¤ëŠ”ê°€?
   - êµ¬ì¡°ê°€ ì‚°ë§Œí•œê°€?

2. **ê°œì„  ê°€ì„¤**: ê°€ì¥ íš¨ê³¼ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ê°œì„ ì•ˆ 1ê°œë¥¼ ì œì•ˆí•˜ì„¸ìš”.
   - ì–´ë–¤ ì„¹ì…˜ì„ ì–´ë–»ê²Œ ìˆ˜ì •í• ì§€
   - ì˜ˆìƒ ê°œì„  íš¨ê³¼ (ì²´ë¥˜ì‹œê°„ ì¦ê°€ìœ¨ %)

3. **Bë²„ì „ ì„¹ì…˜ ìƒì„±**: ê°€ì„¤ì— ë”°ë¼ ìˆ˜ì •ëœ ì„¹ì…˜ì„ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
   - ìˆ˜ì • ëŒ€ìƒ ì„¹ì…˜ë§Œ í¬í•¨
   - ì›ë³¸ê³¼ ë™ì¼í•œ JSON êµ¬ì¡° ìœ ì§€

## ì¶œë ¥ í˜•ì‹ (JSON)
{{
    "problems": [
        {{"section": "intro", "issue": "ë„ì…ë¶€ê°€ ë„ˆë¬´ ì¥í™©í•¨", "severity": "high"}},
        ...
    ],
    "hypothesis": {{
        "target_section": "intro",
        "description": "ë„ì…ë¶€ë¥¼ ì§ˆë¬¸í˜•ìœ¼ë¡œ ì‹œì‘í•˜ê³  í•µì‹¬ ê²°ë¡ ì„ ì²« ë¬¸ì¥ì— ë°°ì¹˜",
        "expected_lift": 15,
        "confidence": "medium"
    }},
    "improved_sections": [
        {{
            "type": "intro",
            "heading": null,
            "content": "ìƒˆë¡œìš´ ë„ì…ë¶€ ë‚´ìš©..."
        }}
    ]
}}
"""

    response = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì½˜í…ì¸  ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        response_format={"type": "json_object"},
        temperature=0.7
    )

    return json.loads(response.choices[0].message.content)


def create_b_version(cursor, article: dict, analysis: dict) -> Optional[str]:
    """
    B ë²„ì „ ê¸€ ìƒì„±

    Returns:
        article_id: ìƒì„±ëœ B ë²„ì „ ID
    """
    original_sections = article.get("sections", [])
    if isinstance(original_sections, str):
        original_sections = json.loads(original_sections)

    improved_sections = analysis.get("improved_sections", [])

    # ì›ë³¸ ì„¹ì…˜ì— ê°œì„ ëœ ì„¹ì…˜ ë³‘í•©
    new_sections = []
    improved_types = {s["type"] for s in improved_sections}

    for section in original_sections:
        if section["type"] in improved_types:
            # ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ êµì²´
            improved = next(s for s in improved_sections if s["type"] == section["type"])
            new_sections.append(improved)
        else:
            new_sections.append(section)

    # B ë²„ì „ INSERT
    article_id = str(uuid.uuid4())

    cursor.execute("""
        INSERT INTO articles (
            id, slug, version, is_active,
            title, description, author, category, tags,
            meta_title, meta_description,
            sections,
            sources, medical_reviewer, reviewed_at,
            image_url, image_alt,
            status, ai_model, prompt_version
        )
        SELECT
            %s, slug, 'B', true,
            title, description, author, category, tags,
            meta_title, meta_description,
            %s,
            sources, medical_reviewer, reviewed_at,
            image_url, image_alt,
            'published', %s, 'ab-test-v1'
        FROM articles
        WHERE slug = %s AND version = 'A'
    """, (
        article_id,
        json.dumps(new_sections, ensure_ascii=False),
        OPENAI_MODEL,
        article["article_slug"]
    ))

    return article_id


def create_ab_test(cursor, article: dict, analysis: dict, b_version_id: str) -> str:
    """A/B í…ŒìŠ¤íŠ¸ ë“±ë¡"""
    test_id = str(uuid.uuid4())
    hypothesis = analysis.get("hypothesis", {})

    cursor.execute("""
        INSERT INTO ab_tests (
            id, article_slug,
            name, hypothesis, target_section,
            control_version, variant_version, traffic_split,
            primary_metric, expected_lift,
            status, started_at
        ) VALUES (
            %s, %s,
            %s, %s, %s,
            'A', 'B', 0.5,
            'avg_time_on_page', %s,
            'running', NOW()
        )
    """, (
        test_id,
        article["article_slug"],
        f"{article['article_slug']} ê°œì„  í…ŒìŠ¤íŠ¸",
        hypothesis.get("description", ""),
        hypothesis.get("target_section", "intro"),
        hypothesis.get("expected_lift", 10)
    ))

    return test_id


def save_analysis(cursor, article: dict, analysis: dict):
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    cursor.execute("""
        INSERT INTO content_analysis (
            article_slug, article_version,
            trigger_type,
            metrics_snapshot,
            problems, hypotheses,
            recommended_action,
            ai_model, analysis_prompt_version
        ) VALUES (
            %s, 'A',
            'scheduled',
            %s,
            %s, %s,
            'run_ab_test',
            %s, 'v1'
        )
    """, (
        article["article_slug"],
        json.dumps({
            "avg_time": article.get("avg_time", 0),
            "avg_bounce": article.get("avg_bounce", 0),
            "avg_engagement": article.get("avg_engagement", 0)
        }),
        json.dumps(analysis.get("problems", []), ensure_ascii=False),
        json.dumps([analysis.get("hypothesis", {})], ensure_ascii=False),
        OPENAI_MODEL
    ))


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš€ AI ì„±ê³¼ ë¶„ì„ ë° A/B í…ŒìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
    print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ: í•˜ìœ„ {UNDERPERFORMING_PERCENTILE}% ê¸€")
    print(f"ğŸ”¬ ìµœëŒ€ í…ŒìŠ¤íŠ¸ ìˆ˜: {MAX_AB_TESTS_PER_RUN}ê°œ")

    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        openai_client = get_openai_client()
        print("âœ… OpenAI ì—°ê²° ì„±ê³µ")

        # ì„±ê³¼ í•˜ìœ„ ê¸€ ì¶”ì¶œ
        print("\nğŸ“‰ ì„±ê³¼ í•˜ìœ„ ê¸€ ì¶”ì¶œ ì¤‘...")
        underperforming = get_underperforming_articles(cursor)

        if not underperforming:
            print("âš ï¸  ë¶„ì„í•  ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ëª¨ë“  ê¸€ì´ ì–‘í˜¸)")
            return

        print(f"   {len(underperforming)}ê°œ ê¸€ ë°œê²¬")

        # ì°¸ê³ ìš© ìƒìœ„ ê¸€
        top_articles = get_top_performing_articles(cursor)
        print(f"   ì°¸ê³ : ìƒìœ„ ê¸€ {len(top_articles)}ê°œ")

        # ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ í…ŒìŠ¤íŠ¸ í™•ì¸
        cursor.execute("""
            SELECT article_slug FROM ab_tests WHERE status = 'running'
        """)
        running_tests = {row[0] for row in cursor.fetchall()}

        created_tests = 0

        for article in underperforming:
            slug = article["article_slug"]

            # ì´ë¯¸ í…ŒìŠ¤íŠ¸ ì¤‘ì´ë©´ ìŠ¤í‚µ
            if slug in running_tests:
                print(f"\nâ­ï¸  {slug} (ì´ë¯¸ í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘)")
                continue

            print(f"\nğŸ” ë¶„ì„ ì¤‘: {slug}")
            print(f"   ì ìˆ˜: {article.get('avg_engagement', 0):.1f} | ì²´ë¥˜: {article.get('avg_time', 0):.1f}ì´ˆ")

            # AI ë¶„ì„
            analysis = analyze_article(openai_client, article, top_articles)

            problems = analysis.get("problems", [])
            hypothesis = analysis.get("hypothesis", {})

            print(f"   ë¬¸ì œì : {len(problems)}ê°œ")
            for p in problems[:2]:
                print(f"      - {p.get('section')}: {p.get('issue')}")

            print(f"   ê°€ì„¤: {hypothesis.get('description', 'N/A')[:50]}...")
            print(f"   ì˜ˆìƒ ê°œì„ : +{hypothesis.get('expected_lift', 0)}%")

            # B ë²„ì „ ìƒì„±
            print("   ğŸ“ B ë²„ì „ ìƒì„± ì¤‘...")
            b_version_id = create_b_version(cursor, article, analysis)

            # A/B í…ŒìŠ¤íŠ¸ ë“±ë¡
            test_id = create_ab_test(cursor, article, analysis, b_version_id)

            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            save_analysis(cursor, article, analysis)

            conn.commit()

            print(f"   âœ… A/B í…ŒìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ID: {test_id[:8]}...)")
            created_tests += 1

        print("\n" + "=" * 50)
        print(f"âœ… ì´ {created_tests}ê°œ A/B í…ŒìŠ¤íŠ¸ ìƒì„±ë¨")

        # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ í…ŒìŠ¤íŠ¸ ìš”ì•½
        cursor.execute("""
            SELECT article_slug, hypothesis, started_at
            FROM ab_tests
            WHERE status = 'running'
            ORDER BY started_at DESC
        """)

        running = cursor.fetchall()
        if running:
            print(f"\nğŸ“Š í˜„ì¬ ì§„í–‰ ì¤‘ì¸ í…ŒìŠ¤íŠ¸: {len(running)}ê°œ")
            for r in running[:5]:
                print(f"   - {r[0]}: {r[1][:40]}...")

        print("\nğŸ‰ ë¶„ì„ ë° A/B í…ŒìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        conn.rollback()
        raise

    finally:
        cursor.close()
        conn.close()


if __name__ == "__main__":
    main()
