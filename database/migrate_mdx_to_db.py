#!/usr/bin/env python3
"""
MDX íŒŒì¼ì„ PostgreSQL DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
- frontmatter â†’ ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼
- ë³¸ë¬¸ â†’ sections JSONB (ì„¹ì…˜ë³„ êµ¬ì¡°í™”)
"""

import re
import json
import uuid
from pathlib import Path
from datetime import datetime
import psycopg2
import yaml

# DB ì—°ê²° ì •ë³´
DB_CONFIG = {
    "host": "34.64.111.186",
    "port": 5432,
    "user": "admin",
    "password": "galddae-password",
    "database": "factcheck_db"
}

# MDX íŒŒì¼ ê²½ë¡œ
MDX_DIR = Path(__file__).parent.parent / "src" / "content" / "articles"


def parse_mdx(file_path: Path) -> dict:
    """MDX íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ frontmatterì™€ ë³¸ë¬¸ ë¶„ë¦¬"""
    content = file_path.read_text(encoding="utf-8")

    # frontmatter ì¶”ì¶œ (--- ì‚¬ì´ì˜ YAML)
    frontmatter_match = re.match(r'^---\s*\n(.*?)\n---\s*\n', content, re.DOTALL)

    if not frontmatter_match:
        raise ValueError(f"No frontmatter found in {file_path}")

    frontmatter_yaml = frontmatter_match.group(1)
    frontmatter = yaml.safe_load(frontmatter_yaml)

    # ë³¸ë¬¸ ì¶”ì¶œ
    body = content[frontmatter_match.end():]

    return {
        "frontmatter": frontmatter,
        "body": body.strip(),
        "slug": file_path.stem  # íŒŒì¼ëª…ì—ì„œ .mdx ì œê±°
    }


def parse_body_to_sections(body: str) -> list:
    """
    ë§ˆí¬ë‹¤ìš´ ë³¸ë¬¸ì„ ì„¹ì…˜ë³„ë¡œ êµ¬ì¡°í™”

    ì„¹ì…˜ íƒ€ì…:
    - intro: ì²« ë²ˆì§¸ ## ì „ê¹Œì§€ì˜ ë‚´ìš©
    - main: ## í—¤ë”© ì„¹ì…˜
    - faq: ### ë¡œ ì‹œì‘í•˜ëŠ” Q&A ì„¹ì…˜
    - conclusion: ê²°ë¡  ì„¹ì…˜
    """
    sections = []

    # ## ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    parts = re.split(r'\n(?=## )', body)

    for i, part in enumerate(parts):
        part = part.strip()
        if not part:
            continue

        # ì²« ë²ˆì§¸ íŒŒíŠ¸ (## ì—†ì´ ì‹œì‘í•˜ë©´ intro)
        if i == 0 and not part.startswith('## '):
            sections.append({
                "type": "intro",
                "heading": None,
                "content": part
            })
            continue

        # ## í—¤ë”© íŒŒì‹±
        heading_match = re.match(r'^## (.+?)(?:\n|$)', part)
        if not heading_match:
            continue

        heading = heading_match.group(1).strip()
        content = part[heading_match.end():].strip()

        # FAQ ì„¹ì…˜ ê°ì§€ (### ê°€ ì—¬ëŸ¬ ê°œ ìˆìœ¼ë©´)
        faq_items = re.findall(r'### (.+?)\n\n(.+?)(?=\n### |\n## |$)', content, re.DOTALL)

        if faq_items and len(faq_items) >= 2:
            sections.append({
                "type": "faq",
                "heading": heading,
                "items": [
                    {"q": q.strip(), "a": a.strip()}
                    for q, a in faq_items
                ]
            })
        elif "ê²°ë¡ " in heading or "ìš”ì•½" in heading:
            sections.append({
                "type": "conclusion",
                "heading": heading,
                "content": content
            })
        elif "í•µì‹¬" in heading and "ìš”ì•½" in heading:
            # í•µì‹¬ ìš”ì•½ì€ bullet pointsë¡œ íŒŒì‹±
            bullets = re.findall(r'^- (.+)$', content, re.MULTILINE)
            sections.append({
                "type": "summary",
                "heading": heading,
                "items": bullets if bullets else [content]
            })
        else:
            sections.append({
                "type": "main",
                "heading": heading,
                "content": content
            })

    return sections


def migrate_article(cursor, parsed: dict) -> str:
    """ë‹¨ì¼ ê¸€ì„ DBì— ì‚½ì…"""
    fm = parsed["frontmatter"]
    sections = parse_body_to_sections(parsed["body"])

    article_id = str(uuid.uuid4())

    # ë‚ ì§œ íŒŒì‹±
    published_at = None
    if fm.get("publishedAt"):
        try:
            published_at = datetime.strptime(fm["publishedAt"], "%Y-%m-%d")
        except:
            pass

    reviewed_at = None
    if fm.get("reviewedAt"):
        try:
            reviewed_at = datetime.strptime(fm["reviewedAt"], "%Y-%m-%d")
        except:
            pass

    # INSERT
    cursor.execute("""
        INSERT INTO articles (
            id, slug, version, is_active,
            title, description, author, category, tags,
            meta_title, meta_description,
            sections,
            sources, medical_reviewer, reviewed_at,
            image_url, image_alt,
            status, ai_model, prompt_version,
            published_at
        ) VALUES (
            %s, %s, %s, %s,
            %s, %s, %s, %s, %s,
            %s, %s,
            %s,
            %s, %s, %s,
            %s, %s,
            %s, %s, %s,
            %s
        )
    """, (
        article_id,
        parsed["slug"],
        "A",  # ê¸°ë³¸ ë²„ì „
        True,
        fm.get("title"),
        fm.get("description"),
        fm.get("author", "36.5 ì˜í•™íŒ€"),
        fm.get("category", "health-info"),
        fm.get("tags", []),
        fm.get("metaTitle"),
        fm.get("metaDescription"),
        json.dumps(sections, ensure_ascii=False),
        fm.get("sources", []),
        fm.get("medicalReviewer"),
        reviewed_at,
        fm.get("image"),
        fm.get("imageAlt"),
        "published",  # ê¸°ì¡´ ê¸€ì€ published ìƒíƒœ
        "manual",  # ìˆ˜ë™ ë§ˆì´ê·¸ë ˆì´ì…˜
        "v0-mdx-import",
        published_at
    ))

    return article_id


def main():
    """ë©”ì¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
    print("ğŸš€ MDX â†’ DB ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    print(f"ğŸ“‚ ì†ŒìŠ¤: {MDX_DIR}")

    # MDX íŒŒì¼ ëª©ë¡
    mdx_files = list(MDX_DIR.glob("*.mdx"))
    print(f"ğŸ“„ ë°œê²¬ëœ íŒŒì¼: {len(mdx_files)}ê°œ")

    # DB ì—°ê²°
    conn = psycopg2.connect(**DB_CONFIG)
    conn.autocommit = True
    cursor = conn.cursor()

    print(f"âœ… DB ì—°ê²° ì„±ê³µ: {DB_CONFIG['database']}")

    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    cursor.execute("SELECT COUNT(*) FROM articles")
    existing_count = cursor.fetchone()[0]
    print(f"ğŸ“Š ê¸°ì¡´ ê¸€ ìˆ˜: {existing_count}")

    success_count = 0
    error_count = 0

    for mdx_file in mdx_files:
        try:
            # sample-articleì€ ìŠ¤í‚µ
            if mdx_file.stem == "sample-article":
                print(f"â­ï¸  {mdx_file.name} (ìƒ˜í”Œ ìŠ¤í‚µ)")
                continue

            parsed = parse_mdx(mdx_file)

            # ì¤‘ë³µ ì²´í¬
            cursor.execute("SELECT id FROM articles WHERE slug = %s AND version = 'A'", (parsed["slug"],))
            if cursor.fetchone():
                print(f"â­ï¸  {mdx_file.name} (ì´ë¯¸ ì¡´ì¬)")
                continue

            article_id = migrate_article(cursor, parsed)
            success_count += 1

            sections_count = len(parse_body_to_sections(parsed["body"]))
            print(f"âœ… {mdx_file.name} â†’ {parsed['slug']} ({sections_count}ê°œ ì„¹ì…˜)")

        except Exception as e:
            error_count += 1
            print(f"âŒ {mdx_file.name}: {e}")

    # ê²°ê³¼ ì¶œë ¥
    cursor.execute("SELECT COUNT(*) FROM articles")
    final_count = cursor.fetchone()[0]

    print("\n" + "=" * 50)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"ğŸ“Š ì´ ê¸€ ìˆ˜: {final_count}ê°œ")

    # ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê¸€ ëª©ë¡ ì¶œë ¥
    cursor.execute("""
        SELECT slug, title, category, array_length(tags, 1) as tag_count,
               jsonb_array_length(sections) as section_count
        FROM articles
        ORDER BY created_at DESC
    """)

    print("\nğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê¸€:")
    print("-" * 80)
    for row in cursor.fetchall():
        print(f"  {row[0]}")
        print(f"    ì œëª©: {row[1][:40]}...")
        print(f"    ì¹´í…Œê³ ë¦¬: {row[2]} | íƒœê·¸: {row[3]}ê°œ | ì„¹ì…˜: {row[4]}ê°œ")
        print()

    cursor.close()
    conn.close()

    print("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
